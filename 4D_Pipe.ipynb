{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step usually contains the following points:\n",
    "\n",
    "1. Obtain\n",
    "2. Clean\n",
    "3. Explore\n",
    "4. Establish baseline outcomes\n",
    "5. Hypothesize solutions\n",
    "\n",
    "The steps that usually require some amount of Python coding are 1, 2 and 3, which is what we will review in the follwing sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First, the use of Pandas to import the (in this case static) files that will be used for analysis. We will probably have to import a complete dataset of a dataset for training and testing separately.\n",
    "\n",
    "```python\n",
    "train_df = pd.read_csv(path_to_file)\n",
    "test_df = pd.read_csv(path_to_file)\n",
    "```\n",
    "\n",
    "Also, we will need to set appart our features and target variable:\n",
    "\n",
    "```python\n",
    "features = ['feature1', 'feature2', ...]\n",
    "target = ['feature_target']\n",
    "\n",
    "train_features = train_df[features]\n",
    "train_target = train_df[target]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some steps can be carried out here, including but not limited to:\n",
    "\n",
    "### Checking head and tail\n",
    "\n",
    "```python\n",
    "train_features.head(10)\n",
    "train_features.tail(10)\n",
    "```\n",
    "\n",
    "### Use of `info()` to see types and details\n",
    "\n",
    "```python\n",
    "train_features.info()\n",
    "```\n",
    "\n",
    "### Check for duplicates\n",
    "\n",
    "```python\n",
    "train_features.duplicated().sum()\n",
    "```\n",
    "\n",
    "This will output the total amount of ducplicated records.\n",
    "\n",
    "### Identify numerical and categorical variables\n",
    "\n",
    "```python\n",
    "train_features.columns\n",
    "\n",
    "numerical = ['feature1', 'feature2', ...]\n",
    "categorical = ['feature10', 'feature11', ...]\n",
    "```\n",
    "\n",
    "This could be further expanded by using more data categories. See [7 Data Types: A Better Way to Think about Data Types for Machine Learning](https://towardsdatascience.com/7-data-types-a-better-way-to-think-about-data-types-for-machine-learning-939fae99a689)\n",
    "\n",
    "### Summarize numerical and categorical variables\n",
    "\n",
    "```python\n",
    "# Summarize numerical\n",
    "train_features.describe(include=[np.number])\n",
    "\n",
    "# Summarize categorical (objects)\n",
    "train_features.describe(include=['O'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the variables\n",
    "\n",
    "### Visualize distribution of target variable\n",
    "\n",
    "```python\n",
    "#Boxplot\n",
    "plt.boxplot(train_target)\n",
    "\n",
    "# Displot\n",
    "sns.displot(train_target)\n",
    "```\n",
    "\n",
    "### Use of IQR rule to detect potential outliers\n",
    "\n",
    "```python\n",
    "target_stats = train_target.describe()\n",
    "\n",
    "# Extract upper and lower bounds\n",
    "IQR = target_stats['75%'] - target_stats['25%']\n",
    "upper = target_stats['75%'] + 1.5 * IQR\n",
    "lower = target_stats['25%'] - 1.5 * IQR\n",
    "```\n",
    "\n",
    "### Slice the dataframe to explore potential outliers\n",
    "\n",
    "Use the upper and lower bound to extract outliers.\n",
    "\n",
    "```python\n",
    "train_target[train_target.outcome < lower]\n",
    "\n",
    "train_target[train_target.outcome > upper]\n",
    "```\n",
    "This process continues as some discoveries about the variables are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of different visualizations for exploring and getting to know the dataset, hopefully inside a function that generalizes this process, as follows:\n",
    "\n",
    "```python\n",
    "def plot_feature(df, col):\n",
    "    '''\n",
    "    Make plot for each featuresleft, the distribution of samples on the feature\n",
    "    right, the dependance of any variable on the target\n",
    "    '''\n",
    "    plt.figure(figsize = (14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if df[col].dtype == 'int64':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    (...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore variables correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful in order to evaluate possibly correlated variables and reduce dimensionality of the dataset to be modeled. Some steps are:\n",
    "\n",
    "### Correlation matrix\n",
    "\n",
    "```python\n",
    "plt.matshow(train_features[numerical].corr(),interpolation='none')\n",
    "```\n",
    "\n",
    "### MDS plot using PCA\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Get correlation matrix\n",
    "similarities = train_features[numerical].corr()\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "corr_pca = pca.fit_transform(similarities)\n",
    "\n",
    "#Plot the matrix\n",
    "_, ax = plt.subplots(1,1)\n",
    "\n",
    "sns.scatterplot(x=corr_pca[:,0], y=corr_pca[:,1], ax=ax)\n",
    "\n",
    "for i, txt in enumerate(train_features[numerical].columns):\n",
    "    ax.annotate(txt, (corr_pca[i,0], corr_pca[i,1]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
